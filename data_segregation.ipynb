{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3171cc12",
   "metadata": {},
   "source": [
    "Pokemon Combat Data Segregation & Feature Engineering\n",
    "\n",
    "This notebook demonstrates how to properly split data for machine learning and engineer features for high performance.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "- 🔄 Create proper train/validation/test splits without data leakage\n",
    "- ⚡ Engineer features that drive 95%+ accuracy\n",
    "- 📊 Understand why certain features are important for Pokemon battles\n",
    "- 🎯 Prepare optimized datasets for model training\n",
    "\n",
    "## What We'll Learn:\n",
    "\n",
    "- GroupShuffleSplit to prevent pair leakage\n",
    "- Advanced feature engineering techniques\n",
    "- Speed and stat ratio importance in Pokemon battles\n",
    "- How to save optimized datasets for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1596c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ae801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Data Segregation & Feature Engineering Pipeline\n",
      "🎯 Goal: Create train/val/test splits without data leakage\n",
      "⚡ Focus: Engineer features for 95%+ accuracy\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for data splitting and feature engineering\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "print(\"📚 Data Segregation & Feature Engineering Pipeline\")\n",
    "print(\"🎯 Goal: Create train/val/test splits without data leakage\")\n",
    "print(\"⚡ Focus: Engineer features for 95%+ accuracy\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54d4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 STEP 1: Loading and Analyzing Clean Data\n",
      "--------------------------------------------------\n",
      "📊 Dataset Overview:\n",
      "   Total battles: 50,000\n",
      "   Unique Pokemon pairs: 46,211\n",
      "   Features per row: 22\n",
      "\n",
      "🎯 Target Distribution Analysis:\n",
      "   Pokemon A (first) wins: 23,601 (47.2%)\n",
      "   Pokemon B (second) wins: 26,399 (52.8%)\n",
      "\n",
      "💡 Why This Distribution Matters:\n",
      "   ✅ Well-balanced: Natural battle dynamics preserved\n",
      "   📈 Good for machine learning: No severe class imbalance\n",
      "\n",
      "🔍 Data Quality Check:\n",
      "   Pair coverage: 0.924 (lower = more repeats)\n",
      "   Average battles per pair: 1.1\n",
      "   Missing values: 48016\n",
      "\n",
      "🎯 Next: Split data to prevent overfitting\n",
      "\n",
      "🔄 STEP 2: Creating Train/Test Split (85%/15%)\n",
      "📚 Why GroupShuffleSplit: Ensures same Pokemon pair doesn't appear in both train and test\n",
      "   Train set: 42,492 battles\n",
      "   Test set: 7,508 battles\n",
      "   Split ratio: 85.0% train, 15.0% test\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Load Clean Data and Understand Distribution\n",
    "print(\"🔍 STEP 1: Loading and Analyzing Clean Data\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load the cleaned data from previous notebook\n",
    "df = pd.read_csv('data/final_cleaned_no_duplicates.csv')\n",
    "\n",
    "print(\"📊 Dataset Overview:\")\n",
    "print(f\"   Total battles: {len(df):,}\")\n",
    "print(f\"   Unique Pokemon pairs: {df['pair_key'].nunique():,}\")\n",
    "print(f\"   Features per row: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n🎯 Target Distribution Analysis:\")\n",
    "a_wins = df['did_a_win'].sum()\n",
    "b_wins = len(df) - a_wins\n",
    "a_win_rate = df['did_a_win'].mean()\n",
    "\n",
    "print(f\"   Pokemon A (first) wins: {a_wins:,} ({a_win_rate:.1%})\")\n",
    "print(f\"   Pokemon B (second) wins: {b_wins:,} ({1-a_win_rate:.1%})\")\n",
    "\n",
    "print(f\"\\n💡 Why This Distribution Matters:\")\n",
    "if 0.45 <= a_win_rate <= 0.55:\n",
    "    print(f\"   ✅ Well-balanced: Natural battle dynamics preserved\")\n",
    "    print(f\"   📈 Good for machine learning: No severe class imbalance\")\n",
    "else:\n",
    "    print(f\"   📊 Imbalanced but natural: Reflects real Pokemon battle patterns\")\n",
    "\n",
    "print(f\"\\n🔍 Data Quality Check:\")\n",
    "pair_coverage = df['pair_key'].nunique() / len(df)\n",
    "avg_battles_per_pair = len(df) / df['pair_key'].nunique()\n",
    "\n",
    "print(f\"   Pair coverage: {pair_coverage:.3f} (lower = more repeats)\")\n",
    "print(f\"   Average battles per pair: {avg_battles_per_pair:.1f}\")\n",
    "print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\n🎯 Next: Split data to prevent overfitting\")\n",
    "\n",
    "# Create train/test split using GroupShuffleSplit to avoid pair leakage\n",
    "print(f\"\\n🔄 STEP 2: Creating Train/Test Split (85%/15%)\")\n",
    "print(\"📚 Why GroupShuffleSplit: Ensures same Pokemon pair doesn't appear in both train and test\")\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(df, groups=df['pair_key']))\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "test_df  = df.iloc[test_idx ].reset_index(drop=True)\n",
    "\n",
    "print(f\"   Train set: {len(train_df):,} battles\")\n",
    "print(f\"   Test set: {len(test_df):,} battles\")\n",
    "print(f\"   Split ratio: {len(train_df)/len(df):.1%} train, {len(test_df)/len(df):.1%} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de65efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- second split: train --> (train, val)  -----------------\n",
    "gss_val = GroupShuffleSplit(n_splits=1, test_size=0.176, random_state=43)\n",
    "tr_idx, val_idx = next(gss_val.split(train_df, groups=train_df['pair_key']))\n",
    "\n",
    "# use the SAME original frame for both selections\n",
    "val_df   = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "train_df = train_df.iloc[tr_idx].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8f2ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Examining our train/validation/test splits\n",
      "This helps us verify that our splits are balanced and properly segregated\n",
      "\n",
      "Train rows: 34995 | positives:0.470 | unique pairs:32365\n",
      "Val   rows:  7497 | positives:0.473 | unique pairs:6914\n",
      "Test  rows:  7508 | positives:0.482 | unique pairs:6932\n",
      "\n",
      "📚 What we're looking for:\n",
      "• Win rates around 0.500 (balanced outcomes)\n",
      "• No overlapping unique pairs between splits\n",
      "• Reasonable size distributions (Train > Val ≈ Test)\n"
     ]
    }
   ],
   "source": [
    "# 📊 Dataset Split Analysis\n",
    "# Let's create a reporting function to validate our splits and understand the distribution\n",
    "\n",
    "def report(part, name):\n",
    "    \"\"\"\n",
    "    Analyze dataset splits\n",
    "    \n",
    "    This function helps us understand:\n",
    "    - Size of each split (number of rows)\n",
    "    - Win rate balance (should be around 50% for fair battles)\n",
    "    - Unique pairs (confirms no data leakage between splits)\n",
    "    \"\"\"\n",
    "    print(f'{name:<5} rows:{len(part):>6} | positives:{part[\"did_a_win\"].mean():.3f} | unique pairs:{part[\"pair_key\"].nunique()}')\n",
    "\n",
    "print(\"🎯 Examining our train/validation/test splits\")\n",
    "print(\"This helps us verify that our splits are balanced and properly segregated\")\n",
    "print()\n",
    "\n",
    "for part, name in [(train_df, 'Train'), (val_df, 'Val'), (test_df, 'Test')]:\n",
    "    report(part, name)\n",
    "\n",
    "print()\n",
    "print(\"📚 What we're looking for:\")\n",
    "print(\"• Win rates around 0.500 (balanced outcomes)\")\n",
    "print(\"• No overlapping unique pairs between splits\")\n",
    "print(\"• Reasonable size distributions (Train > Val ≈ Test)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e5ddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Concept: Data Leakage Prevention\n",
      "Data leakage occurs when information from the test set 'leaks' into training.\n",
      "This would give us artificially high performance that doesn't generalize.\n",
      "\n",
      "📚 Why we check for pair leakage:\n",
      "• If the same Pokemon pair appears in both train and test sets,\n",
      "  the model might memorize specific matchups rather than learn general patterns\n",
      "• This would lead to overoptimistic performance estimates\n",
      "\n",
      "✅ No pair leakage detected - Our splits are properly isolated!\n",
      "\n",
      "🎉 This means our model evaluation will be trustworthy and realistic.\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Data Leakage Validation - Critical Step in ML Pipeline\n",
    "# This is one of the most important validation steps in machine learning!\n",
    "\n",
    "print(\"🎯 Concept: Data Leakage Prevention\")\n",
    "print(\"Data leakage occurs when information from the test set 'leaks' into training.\")\n",
    "print(\"This would give us artificially high performance that doesn't generalize.\")\n",
    "print()\n",
    "\n",
    "print(\"📚 Why we check for pair leakage:\")\n",
    "print(\"• If the same Pokemon pair appears in both train and test sets,\")\n",
    "print(\"  the model might memorize specific matchups rather than learn general patterns\")\n",
    "print(\"• This would lead to overoptimistic performance estimates\")\n",
    "print()\n",
    "\n",
    "# Validate no overlap between splits using set intersection\n",
    "assert not set(train_df.pair_key) & set(val_df.pair_key),  'Leak between train and val'\n",
    "assert not set(train_df.pair_key) & set(test_df.pair_key), 'Leak between train and test'\n",
    "assert not set(val_df.pair_key)   & set(test_df.pair_key), 'Leak between val and test'\n",
    "\n",
    "print('✅ No pair leakage detected - Our splits are properly isolated!')\n",
    "print()\n",
    "print(\"🎉 This means our model evaluation will be trustworthy and realistic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b7404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Saving Clean Dataset Splits\n",
      "We're saving three separate files to maintain clear boundaries:\n",
      "\n",
      "💾 Saved clean datasets:\n",
      "• data/train_set.csv\n",
      "• data/val_set.csv\n",
      "• data/test_set.csv\n",
      "\n",
      "📚 Why separate files matter:\n",
      "• Clear separation prevents accidental mixing during model development\n",
      "• Each file has a specific purpose in the ML workflow\n",
      "• Training set: Learn patterns\n",
      "• Validation set: Tune hyperparameters and select best model\n",
      "• Test set: Final, unbiased performance evaluation\n",
      "\n",
      "🎯 These datasets have:\n",
      "✅ Natural win distribution (balanced outcomes)\n",
      "✅ NO data leakage between splits\n",
      "✅ Proper Pokemon pair segregation\n",
      "\n",
      "🚀 Ready for model training!\n",
      "💾 Saved clean datasets:\n",
      "• data/train_set.csv\n",
      "• data/val_set.csv\n",
      "• data/test_set.csv\n",
      "\n",
      "📚 Why separate files matter:\n",
      "• Clear separation prevents accidental mixing during model development\n",
      "• Each file has a specific purpose in the ML workflow\n",
      "• Training set: Learn patterns\n",
      "• Validation set: Tune hyperparameters and select best model\n",
      "• Test set: Final, unbiased performance evaluation\n",
      "\n",
      "🎯 These datasets have:\n",
      "✅ Natural win distribution (balanced outcomes)\n",
      "✅ NO data leakage between splits\n",
      "✅ Proper Pokemon pair segregation\n",
      "\n",
      "🚀 Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# 💾 Saving Our Clean, Segregated Datasets\n",
    "# Now we'll save our properly split datasets for model training\n",
    "\n",
    "print(\"🎯 Saving Clean Dataset Splits\")\n",
    "print(\"We're saving three separate files to maintain clear boundaries:\")\n",
    "print()\n",
    "\n",
    "# Save each split to a separate CSV file\n",
    "train_df.to_csv('data/train_set.csv', index=False)\n",
    "val_df.to_csv('data/val_set.csv', index=False)\n",
    "test_df.to_csv('data/test_set.csv', index=False)\n",
    "\n",
    "print(\"💾 Saved clean datasets:\")\n",
    "print(\"• data/train_set.csv\")\n",
    "print(\"• data/val_set.csv\") \n",
    "print(\"• data/test_set.csv\")\n",
    "print()\n",
    "\n",
    "print(\"📚 Why separate files matter:\")\n",
    "print(\"• Clear separation prevents accidental mixing during model development\")\n",
    "print(\"• Each file has a specific purpose in the ML workflow\")\n",
    "print(\"• Training set: Learn patterns\")\n",
    "print(\"• Validation set: Tune hyperparameters and select best model\")\n",
    "print(\"• Test set: Final, unbiased performance evaluation\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 These datasets have:\")\n",
    "print(\"✅ Natural win distribution (balanced outcomes)\")\n",
    "print(\"✅ NO data leakage between splits\")\n",
    "print(\"✅ Proper Pokemon pair segregation\")\n",
    "print()\n",
    "print(\"🚀 Ready for model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25c5e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Feature Engineering for Pokemon Battles\n",
      "We'll create features that capture the strategic elements of Pokemon combat\n",
      "\n",
      "📊 Loaded clean datasets:\n",
      "   Train: 34995 rows\n",
      "   Val: 7497 rows\n",
      "   Test: 7508 rows\n",
      "\n",
      "📚 Feature Engineering\n",
      "Instead of using raw stats, we'll create features that capture battle dynamics:\n",
      "\n",
      "✅ Created strategic battle features:\n",
      "• Speed difference (turn order advantage)\n",
      "• Base stat total difference (overall power)\n",
      "• Attack/Defense ratios (offensive capability)\n",
      "• Special move ratios (special attack effectiveness)\n",
      "• HP difference (survivability advantage)\n",
      "\n",
      "🧹 Data type optimization completed\n",
      "\n",
      "📋 Quality Audit - Ensuring ML readiness:\n",
      "Train ✔  rows: 34995  win_rate:0.470\n",
      "Val   ✔  rows:  7497  win_rate:0.473\n",
      "Test  ✔  rows:  7508  win_rate:0.482\n",
      "\n",
      "✅ All features present and ready for modeling\n",
      "\n",
      "🚀 COMPREHENSIVE FEATURE SET:\n",
      "   Numeric features: 23 (stats + engineered)\n",
      "   Categorical features: 8 (types + metadata)\n",
      "\n",
      "📊 Loaded clean datasets:\n",
      "   Train: 34995 rows\n",
      "   Val: 7497 rows\n",
      "   Test: 7508 rows\n",
      "\n",
      "📚 Feature Engineering\n",
      "Instead of using raw stats, we'll create features that capture battle dynamics:\n",
      "\n",
      "✅ Created strategic battle features:\n",
      "• Speed difference (turn order advantage)\n",
      "• Base stat total difference (overall power)\n",
      "• Attack/Defense ratios (offensive capability)\n",
      "• Special move ratios (special attack effectiveness)\n",
      "• HP difference (survivability advantage)\n",
      "\n",
      "🧹 Data type optimization completed\n",
      "\n",
      "📋 Quality Audit - Ensuring ML readiness:\n",
      "Train ✔  rows: 34995  win_rate:0.470\n",
      "Val   ✔  rows:  7497  win_rate:0.473\n",
      "Test  ✔  rows:  7508  win_rate:0.482\n",
      "\n",
      "✅ All features present and ready for modeling\n",
      "\n",
      "🚀 COMPREHENSIVE FEATURE SET:\n",
      "   Numeric features: 23 (stats + engineered)\n",
      "   Categorical features: 8 (types + metadata)\n",
      "\n",
      "💾 SAVED OPTIMIZED DATASETS:\n",
      "   • processed/train.parquet (efficient binary format)\n",
      "   • processed/val.parquet\n",
      "   • processed/test.parquet\n",
      "   • processed/feature_config.json (feature definitions)\n",
      "\n",
      "🎯 DATASETS READY FOR MODEL TRAINING!\n",
      "📚 Next step: Use model_training.ipynb to build and evaluate your Pokemon battle predictor\n",
      "======================================================================\n",
      "💾 SAVED OPTIMIZED DATASETS:\n",
      "   • processed/train.parquet (efficient binary format)\n",
      "   • processed/val.parquet\n",
      "   • processed/test.parquet\n",
      "   • processed/feature_config.json (feature definitions)\n",
      "\n",
      "🎯 DATASETS READY FOR MODEL TRAINING!\n",
      "📚 Next step: Use model_training.ipynb to build and evaluate your Pokemon battle predictor\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Feature Engineering - Creating Powerful Predictive Features\n",
    "# This step transforms raw stats into meaningful battle indicators\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"🎯 Feature Engineering for Pokemon Battles\")\n",
    "print(\"We'll create features that capture the strategic elements of Pokemon combat\")\n",
    "print()\n",
    "\n",
    "# Load our clean, segregated datasets\n",
    "train_df = pd.read_csv('data/train_set.csv')\n",
    "val_df   = pd.read_csv('data/val_set.csv')\n",
    "test_df  = pd.read_csv('data/test_set.csv')\n",
    "\n",
    "print(f\"📊 Loaded clean datasets:\")\n",
    "print(f\"   Train: {len(train_df)} rows\")\n",
    "print(f\"   Val: {len(val_df)} rows\") \n",
    "print(f\"   Test: {len(test_df)} rows\")\n",
    "print()\n",
    "\n",
    "print(\"📚 Feature Engineering\")\n",
    "print(\"Instead of using raw stats, we'll create features that capture battle dynamics:\")\n",
    "print()\n",
    "\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    # Speed difference (crucial for battles - who attacks first?)\n",
    "    frame['speed_diff'] = frame['a_speed'] - frame['b_speed']\n",
    "    \n",
    "    # Base stat totals and difference (overall power comparison)\n",
    "    a_bst = frame[['a_hp','a_attack','a_defense','a_sp_atk','a_sp_def','a_speed']].sum(axis=1)\n",
    "    b_bst = frame[['b_hp','b_attack','b_defense','b_sp_atk','b_sp_def','b_speed']].sum(axis=1)\n",
    "    frame['a_bst'] = a_bst\n",
    "    frame['b_bst'] = b_bst\n",
    "    frame['bst_diff'] = a_bst - b_bst\n",
    "    \n",
    "    # Attack/Defense ratios (offensive vs defensive capabilities)\n",
    "    frame['a_atk_def_ratio'] = frame['a_attack'] / (frame['a_defense'] + 1)  # +1 prevents division by zero\n",
    "    frame['b_atk_def_ratio'] = frame['b_attack'] / (frame['b_defense'] + 1)\n",
    "    frame['atk_def_ratio_diff'] = frame['a_atk_def_ratio'] - frame['b_atk_def_ratio']\n",
    "    \n",
    "    # Special attack/defense ratios (special move effectiveness)\n",
    "    frame['a_sp_ratio'] = frame['a_sp_atk'] / (frame['a_sp_def'] + 1)\n",
    "    frame['b_sp_ratio'] = frame['b_sp_atk'] / (frame['b_sp_def'] + 1)\n",
    "    frame['sp_ratio_diff'] = frame['a_sp_ratio'] - frame['b_sp_ratio']\n",
    "    \n",
    "    # HP advantage (survivability comparison)\n",
    "    frame['hp_diff'] = frame['a_hp'] - frame['b_hp']\n",
    "\n",
    "print(\"✅ Created strategic battle features:\")\n",
    "print(\"• Speed difference (turn order advantage)\")\n",
    "print(\"• Base stat total difference (overall power)\")\n",
    "print(\"• Attack/Defense ratios (offensive capability)\")\n",
    "print(\"• Special move ratios (special attack effectiveness)\")  \n",
    "print(\"• HP difference (survivability advantage)\")\n",
    "print()\n",
    "\n",
    "# Data type optimization and cleaning\n",
    "for frame in (train_df, val_df, test_df):\n",
    "    # Ensure target is numeric (required for ML algorithms)\n",
    "    frame['did_a_win'] = frame['did_a_win'].astype(int)\n",
    "    \n",
    "    # Convert boolean columns to int (ML algorithms prefer numeric)\n",
    "    frame[['a_legendary', 'b_legendary']] = frame[['a_legendary', 'b_legendary']].astype(int)\n",
    "    \n",
    "    # Fill any NaNs in type_2 with 'None' (some Pokemon only have one type)\n",
    "    frame[['a_type_2', 'b_type_2']] = frame[['a_type_2', 'b_type_2']].fillna('None')\n",
    "\n",
    "print(\"🧹 Data type optimization completed\")\n",
    "print()\n",
    "\n",
    "# Quality validation function\n",
    "def quick_audit(df, name):\n",
    "    \"\"\"Validate data quality for machine learning\"\"\"\n",
    "    assert df['did_a_win'].nunique() == 2, f'{name}: target should be binary'\n",
    "    assert df.isna().sum().sum() == 0, f'{name}: no NaN values allowed'\n",
    "    print(f'{name:<5} ✔  rows:{len(df):>6}  win_rate:{df.did_a_win.mean():.3f}')\n",
    "\n",
    "print(f\"📋 Quality Audit - Ensuring ML readiness:\")\n",
    "for d, n in [(train_df, 'Train'), (val_df, 'Val'), (test_df, 'Test')]:\n",
    "    quick_audit(d, n)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define our comprehensive feature set for high-performance modeling\n",
    "high_performance_features = {\n",
    "    'numeric': [\n",
    "        # Original Pokemon stats\n",
    "        'a_hp', 'a_attack', 'a_defense', 'a_sp_atk', 'a_sp_def', 'a_speed',\n",
    "        'b_hp', 'b_attack', 'b_defense', 'b_sp_atk', 'b_sp_def', 'b_speed',\n",
    "        # Strategic engineered features\n",
    "        'speed_diff', 'bst_diff', 'hp_diff',\n",
    "        'a_bst', 'b_bst',\n",
    "        'a_atk_def_ratio', 'b_atk_def_ratio', 'atk_def_ratio_diff',\n",
    "        'a_sp_ratio', 'b_sp_ratio', 'sp_ratio_diff'\n",
    "    ],\n",
    "    'categorical': [\n",
    "        'a_type_1', 'a_type_2', 'b_type_1', 'b_type_2',\n",
    "        'a_generation', 'b_generation', 'a_legendary', 'b_legendary'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verify all features exist in our datasets\n",
    "missing_features = []\n",
    "for feature_type, features in high_performance_features.items():\n",
    "    for feature in features:\n",
    "        if feature not in train_df.columns:\n",
    "            missing_features.append(feature)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"⚠️ Missing features: {missing_features}\")\n",
    "else:\n",
    "    print(f\"✅ All features present and ready for modeling\")\n",
    "\n",
    "print()\n",
    "print(f'🚀 COMPREHENSIVE FEATURE SET:')\n",
    "print(f'   Numeric features: {len(high_performance_features[\"numeric\"])} (stats + engineered)')\n",
    "print(f'   Categorical features: {len(high_performance_features[\"categorical\"])} (types + metadata)')\n",
    "print()\n",
    "\n",
    "# Save optimized datasets in efficient Parquet format\n",
    "Path('processed').mkdir(exist_ok=True)\n",
    "\n",
    "train_df.to_parquet('processed/train.parquet', index=False)\n",
    "val_df.to_parquet('processed/val.parquet', index=False)\n",
    "test_df.to_parquet('processed/test.parquet', index=False)\n",
    "\n",
    "# Save feature configuration for consistent model training\n",
    "feature_config = {\n",
    "    'numeric_features': high_performance_features['numeric'],\n",
    "    'categorical_features': high_performance_features['categorical'],\n",
    "    'target': 'did_a_win',\n",
    "    'description': 'Comprehensive Pokemon battle prediction features',\n",
    "    'pipeline_version': 'educational_v1'\n",
    "}\n",
    "\n",
    "with open('processed/feature_config.json', 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "\n",
    "print(f'💾 SAVED OPTIMIZED DATASETS:')\n",
    "print(f'   • processed/train.parquet (efficient binary format)')\n",
    "print(f'   • processed/val.parquet')\n",
    "print(f'   • processed/test.parquet')\n",
    "print(f'   • processed/feature_config.json (feature definitions)')\n",
    "print()\n",
    "\n",
    "print(f'🎯 DATASETS READY FOR MODEL TRAINING!')\n",
    "print(\"📚 Next step: Use model_training.ipynb to build and evaluate your Pokemon battle predictor\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
